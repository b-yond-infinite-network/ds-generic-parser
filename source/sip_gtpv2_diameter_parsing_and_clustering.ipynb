{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import uuid\n",
    "from itertools import repeat\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import clustering_utils as utils\n",
    "import pcap_generic_parser_helper as parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Run\n",
    "1. Create your project folder in /data/input\n",
    "2. Place your pcap and json files within your project folder\n",
    "3. **Set the project_name variable below**\n",
    "4. Run the Notebook from the start\n",
    "5. Check the parser and clustering output in the csv files:\n",
    "    - Parser all protocols &emsp;./data/output/\\<project_name\\>/all_parser_output.csv\n",
    "    - Parser sip only &emsp;&emsp;&emsp;./data/output/\\<project_name\\>/sip_parser_output.csv\n",
    "    - Clustering &emsp;&emsp;&emsp;&emsp;&emsp;./data/output/\\<project_name\\>/clustered_data.csv\n",
    "6. Check Notebook **clustering_analysis.ipynb** for further insights into clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'example_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = Path.cwd().parent / 'data/input' / project_name\n",
    "output_folder_path = Path.cwd().parent / 'data/output' / project_name\n",
    "json_folder_path = output_folder_path / f'jsonfiles_{uuid.uuid4()}'\n",
    "\n",
    "json_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sip_output_filename = 'sip_parser_output.csv'\n",
    "gtp_output_filename = 'gtp_parser_output.csv'\n",
    "diameter_output_filename = 'diameter_parser_output.csv'\n",
    "gtp_sip_output_csv_filename = 'gtp_sip_parser_output.csv'\n",
    "all_output_csv_filename = 'all_parser_output.csv'\n",
    "clustering_csv_filename = 'clustered_data.csv'\n",
    "\n",
    "num_processors = multiprocessing.cpu_count()\n",
    "p = Pool(processes=num_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pcap files from ./data/input/<project_name>\n",
    "pcap_files = [x for x in input_folder_path.glob('*.pcap')]\n",
    "len(pcap_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pcap files to json\n",
    "result = p.starmap(parser.convert_pcap, zip(repeat(json_folder_path), pcap_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get json files from ./data/input/<project_name> and converted ones \n",
    "json_files = [x for x in input_folder_path.glob('*.json')]\n",
    "json_files = json_files + [x for x in json_folder_path.glob('*.json')]\n",
    "len(json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sip Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse json packets, generates a nest List\n",
    "sip_parsed = p.map(parser.read_parse_sip, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output from original parser and concatenate both\n",
    "sip_df_out_final = pd.DataFrame(data=sip_parsed)\n",
    "sip_df_out_final = sip_df_out_final.fillna(\"*\").set_index(0).rename_axis('pcap').add_prefix('sip ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sip_df_out_final.to_csv(output_folder_path / sip_output_filename)\n",
    "sip_df_out_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTPv2 Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible gtpv2 causes\n",
    "gtp_list_from_tshark = p.map(parser.tshark_aggregate_gtp_cause, pcap_files)\n",
    "gtp_causes = list({val for sublist in gtp_list_from_tshark for val in sublist if val != ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse json packets, generates a nest List\n",
    "gtp_parse_output = p.starmap(parser.read_parse_gtp, zip(json_files, repeat(gtp_causes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output from original parser and concatenate both\n",
    "gtp_df_out_final = pd.DataFrame(data=gtp_parse_output)\n",
    "new_columns = ['pcap', 'n Requests', 'n Responses', 'n unanswered requests'] + ['cause = ' + str(cause) for cause in gtp_causes]\n",
    "gtp_df_out_final = gtp_df_out_final.fillna(\"*\").rename(columns={i: new_columns[i] for i in range(len(new_columns))}) \\\n",
    "    .set_index('pcap').add_prefix('gtpv2 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtp_df_out_final.to_csv(output_folder_path / gtp_output_filename)\n",
    "gtp_df_out_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diameter Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible diameter result codes\n",
    "diameter_list_from_tshark = p.map(parser.tshark_aggregate_diameter_result_code, pcap_files)\n",
    "diameter_result_codes = list({val for sublist in diameter_list_from_tshark for val in sublist if val != ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse json packets, generates a nest List\n",
    "diameter_parse_output = p.starmap(parser.read_parse_diameter, zip(json_files, repeat(diameter_result_codes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output from original parser and concatenate both\n",
    "diameter_df_out_final = pd.DataFrame(data=diameter_parse_output)\n",
    "new_columns = ['pcap', 'n Requests', 'n Responses', 'n unanswered requests'] + ['result code = ' + str(code) for code in diameter_result_codes]\n",
    "diameter_df_out_final = diameter_df_out_final.fillna(\"*\").rename(columns={i: new_columns[i] for i in range(len(new_columns))}) \\\n",
    "    .set_index('pcap').add_prefix('diameter ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diameter_df_out_final.to_csv(output_folder_path / diameter_output_filename)\n",
    "diameter_df_out_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge sip, gtpv2, and diameter Parser Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sip_gtp_df = pd.concat([gtp_df_out_final, sip_df_out_final], axis=1)\n",
    "sip_gtp_df.to_csv(output_folder_path / gtp_sip_output_csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parser_out_df = pd.concat([diameter_df_out_final, gtp_df_out_final, sip_df_out_final], axis=1)\n",
    "all_parser_out_df.to_csv(output_folder_path / all_output_csv_filename)\n",
    "all_parser_out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Parser Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_cols = []\n",
    "static_cols = ('gtpv2 n ', 'gtpv2 cause =', 'diameter n ', 'diameter result code =')\n",
    "label = None\n",
    "replace_none_values = '*'\n",
    "use_encoder = True  # If True -> call one_hot_encoder\n",
    "number_of_clusters = -1  # specify number of cluster. If -1 -> calculate optimal_cluster_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate pcaps\n",
    "all_parser_out_df = all_parser_out_df[~all_parser_out_df.index.duplicated(keep='first')]\n",
    "# drop columns with unique value\n",
    "all_parser_out_df = all_parser_out_df[all_parser_out_df.columns[all_parser_out_df.nunique() > 1]]\n",
    "# replace null values\n",
    "all_parser_out_df = utils.transform_data(all_parser_out_df, replace_none_values)\n",
    "all_parser_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_data = all_parser_out_df[[x for x in all_parser_out_df.columns if x not in ignore_cols]]\n",
    "if use_encoder:\n",
    "    df_static = clustering_data[[x for x in clustering_data.columns if x.startswith(static_cols)]]\n",
    "    df_static_set = set(df_static)\n",
    "    df_dynamic = clustering_data[[x for x in clustering_data.columns if not x in df_static_set]]\n",
    "\n",
    "    clustering_data = utils.one_hot_encoder(df_dynamic)\n",
    "    clustering_data = pd.concat([df_static, clustering_data.set_index(df_static.index)], axis=1)\n",
    "clustering_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if number_of_clusters == -1:\n",
    "    number_of_clusters = utils.optimal_cluster_num(clustering_data)\n",
    "params = {'n_clusters': number_of_clusters, 'init': 'k-means++', 'max_iter': 120, 'n_init': 25, 'random_state': 1}\n",
    "clusterer = KMeans(**params)\n",
    "clusters, silhouette = utils.cluster(clusterer, clustering_data)\n",
    "print('number of clusters {}'.format(number_of_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df, clusters_df, score, percent_mean, silhouette_mean = utils.score_fun(all_parser_out_df, clusters, silhouette, label)\n",
    "print('Silhouette Mean {}'.format(silhouette_mean))\n",
    "all_df.to_csv(output_folder_path / clustering_csv_filename)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
