{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import uuid\n",
    "from itertools import repeat\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import clustering_utils as utils\n",
    "import pcap_generic_parser_helper as parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Run\n",
    "1- Create your project folder in /data/input\n",
    "\n",
    "2- Place your pcap and json files within your project folder\n",
    "\n",
    "3- Set the project_name below\n",
    "\n",
    "4- Run the Notebook from the start\n",
    "\n",
    "5- Check the parser and clustering output:\n",
    "> All parsers output ./data/input/<project_name>/all_parser_output.csv\n",
    "<br>\n",
    "> Sip only parser output ./data/input/<project_name>/sip_parser_output.csv\n",
    "<br>\n",
    "> Clustering ./data/input/<project_name>/clustered_data.csv\n",
    "\n",
    "6- Check the notebook clustering_analysis for further insights into clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'example_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = Path.cwd().parent / 'data/input' / project_name\n",
    "output_folder_path = Path.cwd().parent / 'data/output' / project_name\n",
    "json_folder_path = output_folder_path / f'jsonfiles_{uuid.uuid4()}'\n",
    "\n",
    "json_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sip_output_filename = 'sip_parser_output.csv'\n",
    "gtp_output_filename = 'gtp_parser_output.csv'\n",
    "diameter_output_filename = 'diameter_parser_output.csv'\n",
    "gtp_sip_output_csv_filename = 'gtp_sip_parser_output.csv'\n",
    "all_output_csv_filename = 'all_parser_output.csv'\n",
    "\n",
    "num_processors = multiprocessing.cpu_count()\n",
    "p = Pool(processes=num_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pcap files from the ./data/input/<project_name>\n",
    "pcap_files = [x for x in input_folder_path.glob('*.pcap')]\n",
    "len(pcap_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PCAP to JSON\n",
    "pcaps_list = p.starmap(parser.convert_pcap, zip(repeat(json_folder_path), pcap_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get converted json files + ones in ./data/input/<project_name>\n",
    "json_files = [x for x in input_folder_path.glob('*.json')]\n",
    "json_files = json_files + [x for x in json_folder_path.glob('*.json')]\n",
    "len(json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sip Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse json packets, generates a nest List\n",
    "sip_parsed = p.map(parser.read_parse_sip, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output from original parser and concatenate both\n",
    "sip_df_out_final = pd.DataFrame(data=sip_parsed)\n",
    "sip_df_out_final.fillna(\"*\", inplace=True)\n",
    "sip_df_out_final = sip_df_out_final.set_index(sip_df_out_final.columns[0])\n",
    "sip_df_out_final.index.names = ['pcap']\n",
    "sip_df_out_final = sip_df_out_final.add_prefix('sip ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sip_df_out_final.to_csv(output_folder_path / sip_output_filename)\n",
    "sip_df_out_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTPv2 Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible gtpv2 causes\n",
    "protocols_list_from_tshark = p.map(parser.tshark_aggregate_gtp_cause, json_files)\n",
    "# Split each protocol to a new row\n",
    "values = set()\n",
    "for x in protocols_list_from_tshark:\n",
    "    values.update(x)\n",
    "try:\n",
    "    values.remove('')\n",
    "except KeyError as e:\n",
    "    pass\n",
    "gtp_causes = list(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse json packets, generates a nest List\n",
    "gtp_parse_output = p.starmap(parser.read_parse_gtp, zip(json_files, repeat(gtp_causes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output from original parser and concatenate both\n",
    "gtp_df_out_final = pd.DataFrame(data=gtp_parse_output)\n",
    "gtp_df_out_final.fillna(\"*\", inplace=True)\n",
    "gtp_df_out_final = gtp_df_out_final.set_index(gtp_df_out_final.columns[0])\n",
    "gtp_df_out_final.index.names = ['pcap']\n",
    "gtp_df_out_final = gtp_df_out_final.rename(columns={1: 'n Requests', 2: 'n Responses', 3: 'n unanswered requests'})\n",
    "gtp_df_out_final = gtp_df_out_final.add_prefix('gtpv2 ')\n",
    "\n",
    "for i in range(0, len(gtp_causes)):\n",
    "    gtp_df_out_final = gtp_df_out_final.rename(columns={'gtpv2 ' + str(i + 4): 'gtpv2 cause = ' + str(gtp_causes[i])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtp_df_out_final.to_csv(output_folder_path / gtp_output_filename)\n",
    "gtp_df_out_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diameter Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible diameter result codes\n",
    "diameter_list_from_tshark = p.map(parser.tshark_aggregate_diameter_result_code, json_files)\n",
    "# Split each protocol to a new row\n",
    "values = set()\n",
    "for x in diameter_list_from_tshark:\n",
    "    values.update(x)\n",
    "try:\n",
    "    values.remove('')\n",
    "except KeyError as e:\n",
    "    pass\n",
    "diameter_result_codes = list(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse json packets, generates a nest List\n",
    "diameter_parse_output = p.starmap(parser.read_parse_diameter, zip(json_files, repeat(diameter_result_codes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output from original parser and concatenate both\n",
    "diameter_df_out_final = pd.DataFrame(data=diameter_parse_output)\n",
    "diameter_df_out_final.fillna(\"*\", inplace=True)\n",
    "diameter_df_out_final = diameter_df_out_final.set_index(diameter_df_out_final.columns[0])\n",
    "diameter_df_out_final.index.names = ['pcap']\n",
    "diameter_df_out_final = diameter_df_out_final.rename(columns={1: 'n Requests', 2: 'n Responses', 3: 'n unanswered requests'})\n",
    "diameter_df_out_final = diameter_df_out_final.add_prefix('diameter ')\n",
    "\n",
    "for i in range(0, len(diameter_result_codes)):\n",
    "    diameter_df_out_final = diameter_df_out_final.rename(columns={'diameter ' + str(i + 4): 'diameter result code = ' + str(diameter_result_codes[i])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diameter_df_out_final.to_csv(output_folder_path / diameter_output_filename)\n",
    "diameter_df_out_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge sip, gtpv2, and diameter Parser Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sip_gtp_df = pd.merge(gtp_df_out_final, sip_df_out_final, on='pcap')\n",
    "sip_gtp_df.to_csv(output_folder_path / gtp_sip_output_csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_parser_out_df = diameter_df_out_final.merge(gtp_df_out_final , on='pcap').merge(sip_df_out_final, on='pcap')\n",
    "all_parser_out_df = diameter_df_out_final.join([gtp_df_out_final, sip_df_out_final])\n",
    "all_parser_out_df.to_csv(output_folder_path / all_output_csv_filename)\n",
    "all_parser_out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Parser Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_cols = ['pcap']\n",
    "label = None\n",
    "static_cols = ('gtpv2 n ', 'gtpv2 cause =', 'diameter n ', 'diameter result code =')\n",
    "replace_none = None\n",
    "use_encoder = True  # If True calls one_hot_encoder\n",
    "number_of_clusters = -1  # specify number of cluster. If -1 calculate optimal_cluster_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parser_out_df = all_parser_out_df.reset_index().drop_duplicates()\n",
    "if replace_none is not None:\n",
    "    utils.transform_data(all_parser_out_df, replace_none)\n",
    "all_parser_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_data = all_parser_out_df[[x for x in all_parser_out_df.columns if x not in ignore_cols]]\n",
    "if use_encoder:\n",
    "    df_static = clustering_data[[x for x in clustering_data.columns if x.startswith(static_cols)]]\n",
    "    df_dynamic = clustering_data[[x for x in clustering_data.columns if not x in df_static]]\n",
    "\n",
    "    clustering_data = utils.one_hot_encoder(df_dynamic)\n",
    "    clustering_data = pd.concat([df_static, clustering_data], axis=1)\n",
    "clustering_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if number_of_clusters == -1:\n",
    "    number_of_clusters = utils.optimal_cluster_num(clustering_data)\n",
    "params = {'n_clusters': number_of_clusters, 'init': 'k-means++', 'max_iter': 120, 'n_init': 25, 'random_state': 1}\n",
    "clusterer = KMeans(**params)\n",
    "clusters, silhouette = utils.cluster(clusterer, clustering_data)\n",
    "print('number of clusters {}'.format(number_of_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df, clusters_df, score, percent_mean, silhouette_mean = utils.score_fun(all_parser_out_df, clusters, silhouette, label)\n",
    "print('Silhouette Mean {}'.format(silhouette_mean))\n",
    "all_df.to_csv(output_folder_path / 'clustered_data.csv')\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
